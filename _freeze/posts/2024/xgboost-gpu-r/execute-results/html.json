{
  "hash": "210094e0660896b351633f8de022678b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Accelerating XGBoost with GPU in R'\ndate: '2024-08-06'\ncategories: [R, gpu, xgboost]\ncode-fold: false\ntoc: true\nimage: figures/benchmarks.png\nimage-width: 80%\n---\n\n\n\n# TL;DR\nUpdate NVIDIA drivers, install the pre-built `{xgboost}` library with GPU support, and set `device = \"cuda\"` while `tree_method = \"hist\"`.\n\n# What is XGBoost?\n[XGBoost (Extreme Gradient Boosting)](https://xgboost.readthedocs.io/en/latest/) is a powerful machine learning library designed for efficient and scalable implementation of gradient boosting algorithms. It's a perfect example of a cross-language library, available for Python, Julia, and R. Key features include parallel processing, handling missing values, and regularization to prevent overfitting. [According to the developers](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html#introduction), `{xgboost}` models have been used to win several Kaggle {{< fa brands kaggle >}} competitions!\n\nHowever, the most interesting feature is that `{xgboost}` is one of [the few R packages](https://jaredlander.com/content/2021/09/GPUComputingWithR.html#1) that supports GPU computing with R.\n\n# Installation instructions\nAccording to the official documentation of the `{xgboost}` library, to run GPU-accelerated models in R, one must download and install the [**experimental pre-built binary with GPU support**](https://xgboost.readthedocs.io/en/latest/install.html#r) (i.e., not the CRAN version!). Currently, they exist only for Windows {{< fa brands windows >}} and Linux {{< fa brands linux >}}. They are not built with every release, so it's a good practice to monitor [GitHub's release](https://github.com/dmlc/xgboost/releases) page for new binaries. While I am writing this post, the `2.0.3` version is the latest with GPU support for R.\n\nCurrently, only NVIDIA GPUs [are supported](https://xgboost.readthedocs.io/en/latest/gpu/index.html) and pre-built binaries are available for Windows and Linux. So, in a nutshell, prerequisites are as follows:\n\n::: {.callout-important}\n# Prerequisites:\n\n1. Linux {{< fa brands debian >}} / {{< fa brands ubuntu >}} or Windows {{< fa brands windows >}} machine\n2. NVIDIA videocard\n\n:::\n\nDepending on your operating system, installation instructions may vary. While it is quite straightforward for Windows and Linux, running `{xgboost}` with GPU on WSL2 requires some additional steps.\n\n::: {.panel-tabset}\n## Linux {{< fa brands debian >}} / {{< fa brands ubuntu >}}\nNo additional tinkering is required; just be sure that you have installed [the latest NVIDIA](https://www.nvidia.com/en-us/drivers/unix/) drivers on your system.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboost_url <- \"https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/release_2.0.0/xgboost_r_gpu_linux_82d846bbeb83c652a0b1dff0e3519e67569c4a3d.tar.gz\"\ninstall.packages(xgboost_url, repos = NULL, type = \"source\")\n```\n:::\n\n\n\n## Windows {{< fa brands windows >}}\nMake sure that the NVIDIA drivers are up to date. I found [NVCleaninstall](https://www.techpowerup.com/download/techpowerup-nvcleanstall/) extremely helpful and easy for updating drivers.\n\n![NVCleaninstall screenshot. Only display driver is required.](figures/NVCleanstall_escFw822lQ.png){width=50%}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboost_url <- \"https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/release_2.0.0/xgboost_r_gpu_win64_82d846bbeb83c652a0b1dff0e3519e67569c4a3d.tar.gz\"\ninstall.packages(xgboost_url, repos = NULL, type = \"source\")\n```\n:::\n\n\n\n## WSL2 {{< fa brands linux >}}\nBy default, WSL2 does have access to your GPU. You can check this by running `nvidia-smi` in your terminal. If you see something similar to this, you are all set:\n\n![Checking that WSL2 have access to GPU.](figures/WindowsTerminal_kTF31RPuRA.png){width=50%}\n\n::: {.callout-note collapse=true}\n# CUDA Toolkit\nOne may also be interested in installing the CUDA Toolkit. [This video](https://youtu.be/JaHVsZa2jTc?si=b9juw5AJTfrsiH2z) is a step-by-step tutorial on how to install WSL and the CUDA Toolkit. Be aware that only Debian-based WSL distros are currently supported.\n\n{{< video https://youtu.be/JaHVsZa2jTc?si=b9juw5AJTfrsiH2z\n    aspect-ratio=\"21x9\" \n>}}\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgboost_url <- \"https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/release_2.0.0/xgboost_r_gpu_linux_82d846bbeb83c652a0b1dff0e3519e67569c4a3d.tar.gz\"\ninstall.packages(xgboost_url, repos = NULL, type = \"source\")\n```\n:::\n\n\n\n:::\n\n# Testing GPU support\nActually, the `{xgboost}` developers have [a lot of demo scripts](https://github.com/dmlc/xgboost/tree/master/R-package/demo) showing how to use the R package properly. Below, I am using a slightly updated [`gpu_accelerated.R`](https://github.com/dmlc/xgboost/blob/master/R-package/demo/gpu_accelerated.R) as an example.\n\nWhile there [is some evidence](https://jaredlander.com/content/2021/09/GPUComputingWithR.html#71) that `{tidymodels}` can run `{xgboost}` on GPU, I failed to do so. I had the impression that the issue lies in the `xgb.DMatrix` class, which is required for `{xgboost}` to distribute computations on the GPU properly, but maybe I am wrong.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\" code-summary=\"Generate data for modelling\"}\nlibrary(xgboost)\nlibrary(tictoc)\n\n# Simulate N x p random matrix with some binomial response dependent on pp columns\nset.seed(111)\nN <- 1000000\np <- 50\npp <- 25\nX <- matrix(runif(N * p), ncol = p)\nbetas <- 2 * runif(pp) - 1\nsel <- sort(sample(p, pp))\nm <- X[, sel] %*% betas - 1 + rnorm(N)\ny <- rbinom(N, 1, plogis(m))\n\ntr <- sample.int(N, N * 0.75)\ndtrain <- xgb.DMatrix(X[tr, ], label = y[tr])\ndtest <- xgb.DMatrix(X[-tr, ], label = y[-tr])\nevals <- list(train = dtrain, test = dtest)\n\n# Create parameter sets\nparam <- list(\n  objective = \"reg:logistic\",\n  eval_metric = \"error\",\n  eval_metric = \"logloss\",\n  max_depth = 2,\n  eta = 1\n)\n```\n:::\n\n\n\n::: {.callout-important appearance=\"simple\"}\nThe key thing is, that since `{xgboost}` v2.0.0 one have to use `watchlist` instead of `evals` and specify `device` parameter.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Ordinary model\ntic()\nxgb_norm <- xgb.train(\n  params = param,\n  data = dtrain,\n  watchlist = evals,\n  nrounds = 100,\n  verbose = 0,\n  tree_method = \"hist\"\n)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n6.98 sec elapsed\n```\n\n\n:::\n\n```{.r .cell-code}\n# GPU model\ntic()\nxgb_gpu <- xgb.train(\n  params = param,\n  data = dtrain,\n  watchlist = evals,\n  nrounds = 100,\n  verbose = 0,\n  tree_method = \"hist\",\n  device = \"cuda\"\n)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.36 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nThat is! I have **a ≈6x speed increase** while testing it on WSL2.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"ggplot2 setup\"}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggdist)\n\nlibrary(hrbrthemes)\nlibrary(showtext)\nfont_add_google(\"Ubuntu Condensed\", \"Ubuntu Condensed\")\nshowtext_auto()\nshowtext_opts(dpi = 300)\n\ntheme_set(\n  hrbrthemes::theme_ft_rc(\n    axis = TRUE,\n    base_family = \"Ubuntu Condensed\",\n    base_size = 12,\n    axis_title_size = 10\n  ) +\n    theme(\n      axis.text = element_text(color = \"white\"),\n      axis.title = element_text(color = \"white\"),\n      plot.title = element_text(color = \"white\"),\n      plot.subtitle = element_text(color = \"white\"),\n      plot.caption = element_text(color = \"white\"),\n      legend.text = element_text(color = \"white\"),\n      legend.title = element_text(color = \"white\")\n    )\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Benchmark code\"}\nlibrary(bench)\n\ngpu_benchs <-\n  bench::mark(\n    GPU = xgb.train(\n      params = param,\n      data = dtrain,\n      watchlist = evals,\n      nrounds = 100,\n      verbose = 0,\n      tree_method = \"hist\",\n      device = \"cuda\"\n    ),\n    CPU = xgb.train(\n      params = param,\n      data = dtrain,\n      watchlist = evals,\n      nrounds = 100,\n      verbose = 0,\n      tree_method = \"hist\"\n    ),\n    relative = FALSE,\n    check = FALSE,\n    iterations = 10L\n  )\n\ntibble(\n  device = rep(c(\"GPU\", \"CPU\"), each = length(gpu_benchs$time[[1]])),\n  time = unlist(gpu_benchs$time)\n) |>\n  ggplot(aes(y = device, x = time)) +\n  stat_pointinterval(\n    aes(color = device),\n    show.legend = FALSE\n  ) +\n  scale_color_manual(values = c(\"#dd1c05\", \"#ebcc2a\")) +\n  scale_x_continuous(\n    breaks = scales::pretty_breaks(n = 6)\n  ) +\n  labs(\n    x = \"Time elapsed, sec\",\n    y = \"Device\"\n  )\n```\n\n::: {.cell-output-display}\n![](xgboost-gpu-r_files/figure-html/benchmarks-1.png){width=672}\n:::\n:::\n\n\n\n# BONUS: Kaggle {{< fa brands kaggle >}} notebooks\nThe Kaggle kernel already comes equipped with the `{xgboost}` library from CRAN. Therefore, the pre-installed `{xgboost}` **does not have GPU** support. First, we need to initialize `renv` to isolate our notebook environment. Using `renv` is the easiest but not the fastest way to do this, as we will need to download and install all the other packages required for our modeling, which may take some time.\n\nApart from that, running GPU-accelerated models is not that different from a Linux server. Just don't forget to set `device=\"cuda\"`, enable `GPU T4×2` acceleration support, and have fun!\n\n::: {.callout-tip}\n# Example kaggle {{< fa brands kaggle >}} notebook\n[https://www.kaggle.com/code/anatoliitsyplenkov/gpu-accelerated-xgboost-in-r](https://www.kaggle.com/code/anatoliitsyplenkov/gpu-accelerated-xgboost-in-r)\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install {renv} package\ninstall.packages(\"renv\")\n\n# Initialise {renv} environment\nrenv::init()\nrenv::activate()\n\n# Install {xgboost} dependencies\nrenv::install(c(\"jsonlite\", \"data.table\"))\n\n# Install the {xgboost} package\nxgboost_url <- \"https://s3-us-west-2.amazonaws.com/xgboost-nightly-builds/release_2.0.0/xgboost_r_gpu_linux_82d846bbeb83c652a0b1dff0e3519e67569c4a3d.tar.gz\"\ninstall.packages(xgboost_url, repos = NULL, type = \"source\")\n```\n:::",
    "supporting": [
      "xgboost-gpu-r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}