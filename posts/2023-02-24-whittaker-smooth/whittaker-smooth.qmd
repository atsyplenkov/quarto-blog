---
title: 'Smoothing high-freq hydrology data'
date: '2023-02-24'
categories: [R, hydrology]
code-fold: true
execute:
  echo: true
  warning: false
  message: false
draft: true
editor_options: 
  chunk_output_type: console
---

# TL;DR
...

# Prerequisites

```{r}
#| label: libraries
#| code-fold: show

library(dplyr)
library(tibble)
library(purrr)
library(ggplot2)
library(pracma)
library(lubridate)
library(atslib)

theme_set(atslib::theme_hp())
```

```{r}
#| label: custom-functions
library(xts)
library(cols4all)
library(dygraphs)

# Interactive plot using {dygraphs} library
.multi_plot <- 
  function(
    .data, # dataframe
    .x, # X-axis variable 
    .y, # Y-axis variables
    .ylab = "Water discharge, cms",
    .col_pal,
    .c4a_pal = "tol.light"
  ){
    
    if (missing(.col_pal)) {
      
      .y_len <-
        base::length(.y)
      .y_colors <-
        cols4all::c4a(palette = .c4a_pal, n = .y_len)
      
    } else {
      
      .y_colors <- .col_pal
      
    }
    
    .data_y1 <-
      .data[, .y]
    
    .data_datetime <-
      dplyr::pull(.data[, .x], 1)
    
    xts::xts(
      x = .data_y1,
      order.by = .data_datetime
    ) %>%
      dygraphs::dygraph(ylab = .ylab) %>%
      dygraphs::dyOptions(useDataTimezone = TRUE) %>%
      dygraphs::dyOptions(colors = .y_colors)
  }

# Split dataframe into list every n rows
# inspired by https://stackoverflow.com/a/75299314/9300556
.split_df <- 
  function(.data, n = 100){
    
    nr <- 
      base::nrow(.data)
    
    base::split(
      .data, 
      base::gl(base::ceiling(nr / n), n, nr)
    )
    
  }

# Whittaker smoother
.whittaker_smooth <- 
  function(.data, .var, ...){
    
    smoothed_df <- 
      .data %>% 
      dplyr::mutate(
        smoothed = pracma::whittaker({{.var}}, ...)
      )
    
    return(smoothed_df)
    
  }



```

# Data exploration
For demonstration purposes, I will use a water discharge dataset from a small urban river. It is a 30-min frequency water discharge values ($Q, m^3Â·s^{-1}$) recalculated from water level loggers ($H, m$) using $Q = f(H)$ relationship curves. The data is quite noisy, and it must be filtered and cleaned for further aggregation. Happy me, there are not many extreme outliers, mostly noise and logger errors.

```{r}
#| label: data-load
#| out-width: 90%

# df <- readRDS("posts/2023-02-24-whittaker-smooth/df.rds")
df <- readRDS("df.rds")

# plot
df %>% 
  .multi_plot(.x = "datetime", .y =  "Q")

```

The dataset covers `807` days and has `38745` rows. Water discharge varies from 0.89 to 33.2 cms. If you zoom in on the graph above, you will see the data's noise.

```{r}
#| label: data-skim

skimr::skim(df)

```

# Smoothing
When smoothing and filtering hydrological data, the running mean or running median is usually applied^[Rodda HJE, Little MA. 2015. Understanding Mathematical and Statistical Techniques in Hydrology: An Examples-Based Approach . John Wiley & Sons, Ltd: Chichester, UK Available from: http://doi.wiley.com/10.1002/9781119077985]. Such linear filters are simple and fast, but create two additional problems owing to their nature: "flat top" and peak shifting. The peak shifting issue is common for most rolling-based filters, and it is almost impossible to eliminate it. Peak slicing and the formation of a flat top are something we can tackle.


```{r}
#| label: whittaker-introduction
#| code-fold: show
#| out-width: 90%
#| column: body

tictoc::tic()

df %>% 
  head(n = 1000) %>% 
  mutate(
    Q_default = pracma::whittaker(Q),
    Q_whittaker = pracma::whittaker(Q, lambda = 100, d = 5),
    Q_runmed = runmed(Q, k = 7, "keep")
  ) %>% 
  .multi_plot(
    .x = "datetime",
    .y =  c("Q", "Q_whittaker", "Q_runmed"),
    .col_pal = c("#7cadde", "#000", "#ed4343")
  )

tictoc::toc()
```

Well, when it comes to speed `whittaker` function is definitely not a hero. It took 2 s to smooth the 1000 rows. Not that bad, but when I applied the filter to the whole dataset of 38 000 rows, it froze my machine for approximately an hour. Obviously, the larger the dataset, the longer it takes to smooth.

```{r}
#| label: whittaker speed
#| out-width: 100%
#| fig-format: svg
foo <- 
  function(n){
    
    start <- Sys.time()
    
    sm <- 
      pracma::whittaker(
      head(df$Q, n = n),
      lambda = 100, 
      d = 5)
    
    end <- Sys.time()
  
    as.numeric(difftime(end, start, units = "secs"))
    
  }

test_df <- 
  tibble(
    seq_n = c(1, 10, 100, 250, 500, 
              750, 1000, 1500)
  ) %>% 
  mutate(time_elapsed = map_dbl(seq_n, ~foo(.x)))

test_df %>% 
  ggplot(aes(x = seq_n,
             y = time_elapsed)) +
  geom_line() +
  labs(
    x = "Number of rows",
    y = "Time elapsed, sec"
  )

```

However, at small datasets `pracma::whittaker` is blazing-fast. Therefore, what if we split the dataset into parts (blocks) and apply the filter to every part consequentially?

First of all, would it speed up the process? Second, is it memory efficient? Finally, what about the data loss in block junctions? Let us run several tests on the toy dataset. I have created two functions: one applies original `pracma::whittaker()` function to a 1000-row dataframe, another splits a dataframe in 500-row chunks and applies the filter for every block separately.

```{r}
#| label: memory allocated

# run filter by splitting dataset into two
# datasets with 500 rows each
splitted <- 
  function(){
    df %>% 
      slice_head(n = 1000) %>% 
      .split_df(n = 500) %>% 
      map_dfr(~.whittaker_smooth(.x, Q, lambda = 100, d = 5))
  }

# run filter as intented
original <- 
  function(){
    
    df %>% 
      slice_head(n = 1000) %>% 
      .whittaker_smooth(Q, lambda = 100, d = 5)
    
  }

bench::mark(
  original(),
  splitted(),
  iterations = 10,
  time_unit = "s",
  check = F
)

```

Okay, so splitting a dataset into pieces consumes two times less memory and performs four times faster! The problem with the split dataset was in block junctions. The graph below shows that the "split" data become slightly wavy at the end of $block_i$ and the beginning of $block_{i+1}$. It is assumed that such soft-filter parameters ($d = 100$ and $\lambda=5$) do not make a significant difference for the smoothing. However, with increased $\lambda$ and order of difference $d$ things may change.

```{r}
#| label: comparing blocks and not
df_compare <- 
  original() %>% 
  mutate(Q_splitted = splitted()$smoothed)

df_compare %>% 
  .multi_plot(
    .x = "datetime",
    .y = c("Q", "smoothed", "Q_splitted"),
    .col_pal = c("#7cadde", "#ed4343", "#000")
  ) %>% 
  dyRangeSelector(
    dateWindow = c("2020-01-10 14:00:00", "2020-01-11 14:30:00")
  )

```

Therefore, splitting the dataset into blocks has several consequences. For my needs, it is nothing, but it is definitely worth using as few blocks as possible. Thus, let us determine the optimum block size!

```{r}
#| label: block size testing
#| fig-dpi: 500
#| out-width: 100%
#| fig-format: svg
foo_block <- 
  function(n){
    
    start <- Sys.time()
    
    df_smoothed <- 
      df %>% 
      slice_head(n = 2000) %>% 
      .split_df(n = n) %>% 
      map_dfr(~.whittaker_smooth(.x, Q, lambda = 100, d = 5))
    
    end <- Sys.time()
  
    as.numeric(difftime(end, start, units = "secs"))
    
  }

test_block <- 
  tibble(
    seq_n = seq(100, 2000, by = 100)
  ) %>% 
  mutate(time_elapsed = map_dbl(seq_n, ~foo_block(.x)))

test_block %>% 
  ggplot(aes(x = seq_n,
             y = time_elapsed)) +
  geom_line() +
  scale_x_continuous(breaks = seq(0, 2000, by = 200)) +
  labs(
    x = "Number of rows in a block",
    y = "Time elapsed, sec"
  )
```

I would say that 800 rows are perfect, but one can enlarge the block size by up to 1200-1600. After that, there was a sharp increase in time consumption.




